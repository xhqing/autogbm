{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28831, 20) (12357, 20)\n",
      "Epoch=0, evaluation: nauc_score=0.8315221260669037, acc_score=0.8544452880443759\n",
      "Epoch=1, evaluation: nauc_score=0.8528100243330181, acc_score=0.8706310375066253\n",
      "Epoch=2, evaluation: nauc_score=0.8613518352393794, acc_score=0.8755758319352994\n",
      "Epoch=3, evaluation: nauc_score=0.8698446147245253, acc_score=0.8811991538658159\n",
      "Epoch=4, evaluation: nauc_score=0.8746073792154596, acc_score=0.8820709692433889\n",
      "Epoch=5, evaluation: nauc_score=0.8810370776950558, acc_score=0.8827393702387096\n",
      "Epoch=6, evaluation: nauc_score=0.887419558244358, acc_score=0.8833887290043773\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "from autotab.metrics import autodl_auc, accuracy\n",
    "from autotab.utils.util import get_solution\n",
    "from autotab.auto_ingestion.dataset import AutoDLDataset\n",
    "from autotab.auto_ingestion import data_io\n",
    "import os\n",
    "from autotab.auto_models.auto_tabular.utils.eda import AutoEDA\n",
    "\n",
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, metadata):\n",
    "        self.done_training = False\n",
    "        self.metadata = metadata\n",
    "\n",
    "        self.metadata_info = metadata.metadata_\n",
    "        self.train_loop_num = 0\n",
    "\n",
    "        self.auto_eda = AutoEDA()\n",
    "\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "\n",
    "        self.pre_increament_preds = True\n",
    "\n",
    "        self.X_test = None\n",
    "\n",
    "        self.next_element = None\n",
    "\n",
    "        self.lgb_info = {}\n",
    "\n",
    "        self.imp_cols = None\n",
    "\n",
    "        self.models = {}\n",
    "\n",
    "        self.sample_cols = None\n",
    "\n",
    "        self.unknow_cols = None\n",
    "\n",
    "        self.first_preds = False\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "        self.keep_training_booster = False\n",
    "    \n",
    "    def simple_lgb(self, X, y, test_x):\n",
    "        self.params = {\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"objective\": \"multiclass\",\n",
    "            'num_class': 2,\n",
    "            \"metric\": \"multi_logloss\",\n",
    "            \"verbosity\": -1,\n",
    "            \"seed\": 2020,\n",
    "            \"num_threads\": 4,\n",
    "        }\n",
    "\n",
    "        self.hyperparams = {\n",
    "            'num_leaves': 31,\n",
    "            'max_depth': -1,\n",
    "            'min_child_samples': 20,\n",
    "            'max_bin': 110,\n",
    "            'subsample': 1,\n",
    "            'subsample_freq': 1,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'min_child_weight': 0.001,\n",
    "            'min_split_gain': 0.02,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 0.1,\n",
    "            \"learning_rate\": 0.1\n",
    "        }\n",
    "\n",
    "        self.train_loop_num += 1\n",
    "        \n",
    "        if self.train_loop_num == 1:\n",
    "            lgb_train = lgb.Dataset(X, y)\n",
    "            self.model = lgb.train({**self.params, **self.hyperparams}, train_set=lgb_train, num_boost_round=10)\n",
    "            preds = self.model.predict(test_x)\n",
    "            self.log_feat_importances()\n",
    "        else:\n",
    "            lgb_train = lgb.Dataset(X[self.imp_cols], y)\n",
    "        \n",
    "            num_boost_round = 10 + self.train_loop_num * 5\n",
    "            num_boost_round = min(40, num_boost_round)\n",
    "        \n",
    "            model = lgb.train({**self.params, **self.hyperparams}, train_set=lgb_train, num_boost_round=num_boost_round)\n",
    "            preds = model.predict(test_x[self.imp_cols])\n",
    "            \n",
    "        return preds\n",
    "\n",
    "    def log_feat_importances(self):\n",
    "        importances = pd.DataFrame({'features': [i for i in self.model.feature_name()], 'importances': self.model.feature_importance(\"gain\")})\n",
    "        importances.sort_values('importances', ascending=False, inplace=True)\n",
    "\n",
    "        importances = importances[importances['importances'] > 0]\n",
    "        size = int(len(importances)*0.8)\n",
    "        if self.imp_cols is None:\n",
    "            if self.unknow_cols is not None:\n",
    "                self.imp_cols = self.unknow_cols + [int(col) for col in importances['features'].values]\n",
    "            else:\n",
    "                self.imp_cols = [int(col) for col in importances['features'].values]\n",
    "        else:\n",
    "            self.imp_cols = [int(col) for col in importances['features'].values]\n",
    "        self.lgb_info['imp_cols'] = self.imp_cols\n",
    "\n",
    "# ========\n",
    "df = pd.read_csv(\"example_data/bank-additional-full.csv\", sep=\";\")\n",
    "\n",
    "trans_cols = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"day_of_week\",\n",
    "              \"poutcome\", \"y\"]\n",
    "\n",
    "for col in trans_cols:\n",
    "    lbe = LabelEncoder()\n",
    "    df[col] = lbe.fit_transform(df[col])\n",
    "\n",
    "label = deepcopy(df[\"y\"])\n",
    "data = deepcopy(df.drop('y', axis=1))\n",
    "train_data, test_data, train_label, test_label = train_test_split(pd.DataFrame(data.values), pd.Series(label.values), test_size=0.3, random_state=1024)\n",
    "print(train_data.shape, test_data.shape)\n",
    "# =====\n",
    "formatted_dir = \"formatted_data\"\n",
    "new_dataset_dir = formatted_dir + \"/\" + os.path.basename(formatted_dir)\n",
    "datanames = data_io.inventory_data(new_dataset_dir)\n",
    "basename = datanames[0]\n",
    "\n",
    "D_train = AutoDLDataset(os.path.join(new_dataset_dir, basename, \"train\"))\n",
    "\n",
    "model = Model(D_train.get_metadata())\n",
    "solution = get_solution(solution_dir=\"formatted_data/formatted_data\")\n",
    "for i in range(50):\n",
    "    if i == 0:\n",
    "        sample_num = 500\n",
    "    elif i == 1:\n",
    "        sample_num = 1000\n",
    "    elif i == 2:\n",
    "        sample_num = 2000\n",
    "    elif i == 3:\n",
    "        sample_num = 3000\n",
    "    else:\n",
    "        sample_num += 500*2**(i-2)\n",
    "\n",
    "   \n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    train_label.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    if sample_num <= train_data.shape[0]:\n",
    "        lgb_data = deepcopy(train_data.loc[:sample_num-1,:])\n",
    "        lgb_label = deepcopy(train_label.loc[:sample_num-1])\n",
    "        \n",
    "        y_pred = model.simple_lgb(lgb_data, lgb_label, test_data)\n",
    "        nauc_score = autodl_auc(solution=solution, prediction=y_pred)\n",
    "        acc_score = accuracy(solution=solution, prediction=y_pred)\n",
    "        print(\"Epoch={}, evaluation: nauc_score={}, acc_score={}\".format(i, nauc_score, acc_score)) \n",
    "    else:\n",
    "        break\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
